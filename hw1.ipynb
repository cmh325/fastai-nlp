{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due on: 5/30.  Please upload your completed assignment to Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For the following words: logistic, logistics, shoe, shoes\n",
    "\n",
    "   a. Porter stem with nltk\n",
    "   \n",
    "   b. lemmatize with nltk\n",
    "   \n",
    "   c. lemmatize with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Chiranth.Hegde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import stem\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = ['logistic','logistics','shoe','shoes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = stem.WordNetLemmatizer()\n",
    "porter = stem.porter.PorterStemmer()\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "lookups = Lookups()\n",
    "spy = Lemmatizer(lookups=lookups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic', 'logistics', 'shoe', 'shoe']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wnl.lemmatize(w) for w in wl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logist', 'logist', 'shoe', 'shoe']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(w) for w in wl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic', 'logistics', 'shoe', 'shoes']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[spy.lookup(w) for w in wl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. n-grams are an important NLP concept.  An n-gram is a contiguous sequence of n items (where the items can be characters, syllables, or words).  Here, we  A 1-gram is a unigram, a 2-gram is a bigram, and a 3-gram is a trigram.\n",
    "\n",
    "Here, we are referring to sequences of words. The sentence \"It was a bright cold day in April.\" contains the following trigrams:\n",
    "\n",
    "- It was a\n",
    "- was a bright\n",
    "- a bright cold\n",
    "- bright cold day\n",
    "- cold day in\n",
    "- day in April\n",
    "\n",
    "Write a function that returns a dictionary with the n-grams of a text (for `min_n <= n <= max_n`) and a count of how often they appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(inp, min_n, max_n):\n",
    "    \n",
    "    #Exercise: FILL IN METHOD\n",
    "    word_list = inp.split(' ') # get the words form the sentence\n",
    "    grams={}\n",
    "    # loop for each N of the N-grams requested\n",
    "    if min_n>=1 and max_n<= len(word_list): # check that makes sense\n",
    "        for i in range(min_n,max_n+1):\n",
    "    #         print(i)        \n",
    "            #for each length of an n-gram we need to return all distinct grams\n",
    "            # easy for n=1 : return all words\n",
    "\n",
    "            words_grams = make_grams(word_list,i) # function to get all the words from the sentence in N-gram order\n",
    "\n",
    "            # check if word in dict -> increase count; else add in dict\n",
    "            for w in words_grams:\n",
    "                if w in grams:\n",
    "                    grams[w]+=1\n",
    "                else:\n",
    "                    grams[w]=1\n",
    "    else:\n",
    "        print('invalid error')\n",
    "        \n",
    "    \n",
    "    return grams\n",
    "\n",
    "def make_grams(word,n):\n",
    "    \"\"\"\n",
    "    Function that Puts 'n' words together to be used in an N-gram calculator.\n",
    "\n",
    "    \"\"\"\n",
    "    w=[]\n",
    "    for i in range(0,len(word)-n+1):\n",
    "        w.append(\" \".join(word[i:i+n]))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence= 'Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polytope where this function has the smallest (or largest) value if such a point exists.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ngrams(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Write a method that given a list of strings (you can think of each string as a document), returns a dictionary for each string, where the keys are the vocabulary words, and the values are the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_frequency(list_of_strings):\n",
    "    # one way to do this would be to to build a library while adding these strings into the vocab, keeping 2 dicts, vocab and string\n",
    "    #makes more sense if vocab is given\n",
    "    vocab_dict={}\n",
    "    for doc in list_of_strings:\n",
    "        vocab_sub={}\n",
    "        words_in_doc = doc.split(\" \")\n",
    "        for i in range(0,len(words_in_doc)):\n",
    "            if words_in_doc[i] in vocab_dict:\n",
    "                #add to vocab\n",
    "                vocab_dict[words_in_doc]+=1\n",
    "            else:\n",
    "                vocab_dict[words_in_doc]+=1\n",
    "\n",
    "            if words_in_doc[i] in vocab_sub:    \n",
    "                vocab_sub[words_in_doc]+=1\n",
    "            else:\n",
    "                vocab_sub[words_in_doc]=1\n",
    "        list_of_dicts.append(vocab_sub)\n",
    "    return list_of_dicts,vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Write a method that when given a list of strings (you can think of each string as a document), calculates the TF-IDF, and returns a term-document matrix with the results. It will be useful to use your `get_vocab_frequency` method from problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the idf term for each word in global vocab\n",
    "# calc tf easy based on word within doc\n",
    "# calc idf globally or all docs\n",
    "\n",
    "def get_tfidf(list_of_strings):\n",
    "    \n",
    "    \n",
    "    \n",
    "    list_of_dicts,vocab_dict = get_vocab_frequency(list_of_strings) \n",
    "    \n",
    "    tfidf=[]\n",
    "    idf_freq={}\n",
    "    # d cal for idf\n",
    "    for word in vocab_dict:\n",
    "        idf_freq[word]=0\n",
    "        for docs_dict in list_of_dicts:\n",
    "            if word in doc_dict:\n",
    "                idf_freq[word]+=1\n",
    "\n",
    "    for doc_dict in list_of_dicts:\n",
    "        tfidf_sub={}\n",
    "        for word in doc_dict:\n",
    "            tf= np.log(1+doc_dict.get(word,0))\n",
    "            idf= np.log(len(list_of_strings)/idf_freq[word])\n",
    "            tf_idf_sub = tf*idf\n",
    "        tfidf.append(tfidf_sub)\n",
    "        \n",
    "    return tfidf\n",
    "\n",
    "def get_tf(word,doc_vocab,main_vocab):\n",
    "    \n",
    "    tf = 0.5 + 0.5*(doc_vocab[word]/main_vocab(word))\n",
    "    \n",
    "    return tf\n",
    "\n",
    "def get_idf(word,doc_vocab,main_vocab):\n",
    "    \n",
    "    n_times= for i in \n",
    "    \n",
    "    idf = np.log(len(list_of_strings)/n_times)\n",
    "    \n",
    "    return idf\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Who said the following (*Hint: Be sure to read the class notebooks and relevant links*):\n",
    "\n",
    "A. \"It's true there's been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success ... which I think is novel in the history of science. It interprets success as approximating unanalyzed data.\"\n",
    "\n",
    "B. \"I agree that it can be difficult to make sense of a model containing billions of parameters. Certainly a human can't understand such a model by inspecting the values of each parameter individually. But one can gain insight by examing the properties of the model—where it succeeds and fails, how well it learns as a function of data, etc.\"\n",
    "\n",
    "C. The big-data big-compute paradigm of modern Deep Learning has in fact “perverted the field” (of computational linguistics) and “sent it off-track”\n",
    "\n",
    "D. Language is crucial to general intelligence, because language is the conduit by which individual intelligence is shared and transformed into societal intelligence.\n",
    "\n",
    "E. Structure is a “necessary evil”, and warned that imposing structure requires us to make certain assumptions, which are invariably wrong for at least some portion of the data, and may become obsolete within the near future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
